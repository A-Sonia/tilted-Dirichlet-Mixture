---
title: "Numerical comparison: Tilted Dirichlet mixture"
output: html_document
date: "2025"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,fig.width = 12)
setwd("~/Dropbox/tDM/data-raw")
save.path="~/Dropbox/tDM/data-raw"

devtools::load_all()
```

```{r,include=FALSE}
library(fGarch)
library(evd)
library(zoo)
library(colorspace)
library(ggplot2)
library(mcclust)
library(tidyverse)
#devtools::install_github("sarawade/mcclust.ext")
library(mcclust.ext)
library(corrplot)
```

```{r,include=FALSE}

data_processing=function(plt=TRUE)
{
  
  
  data1 <- read.csv(file.path("~/Library/Mobile Documents/com~apple~CloudDocs/tDMJune25/FinancialData","data1.csv"))
  data1=data1[,c('Date','S.P.Technology','S.P.Health.Care','S.P.Industrial','S.P.Energy','S.P.Financial','S.P.Materials')]
  missing=apply(data1,1,function(x) any(is.na(x)))
  D=ncol(data1)-1
  Returns=data.frame(as.Date(data1[-1,1]))
  for(d in 1:D)
  {
    Returns=cbind(Returns,as.numeric(-diff(log(data1[,1+d]))))
  }
  
  
  colnames(Returns)=colnames(data1)
  
  
  N=nrow(Returns)
  X0=U0=Z0=c()
  
  par(mfrow=c(2,3),
      pty='s',mgp=c(2,1,0),
      omi=c(0,0,0,0)+0.1,
      mar=c(3,3,2,0),
      cex=1.2)
  for(d in 1:D){
    fit3=  garchFit(~arma(1,0)+garch(1,1),Returns[,1+d],cond.dist='std',trace=FALSE) 
    nu=coef(fit3)['shape']
    se=fit3@fit$se.coef['shape']
    if(plt){
      plot(fit3,which=13,main='',pch=20)
      text(-2.5,4,colnames(Returns)[d+1])
      text(-2.5,3,bquote(hat(nu)==.(round(nu,2))~'('~.(round(se,2))~')'))
    }
    X0=cbind(X0,residuals(fit3))
    U0=cbind(U0,pt(residuals(fit3,standardize=TRUE),nu))
    Z0=cbind(Z0,qfrechet(pt(residuals(fit3,standardize=TRUE),nu)))
  }
  
  
  R0=rowSums(Z0)
  r0=quantile(R0,0.97)
  W0=Z0/rowSums(Z0)
  I0=(R0>=r0)
  if(plt){
    par(mfrow=c(3,2),
        pty='m',
        mgp=c(1,0.5,0),
        omi=c(0,0,0.2,0)+0.1,mar=c(2,3,1,0),
        cex=1.2)
    for( d in 1:D){
      plot(Returns[,1],log(Z0[,d]),
           col=alpha(8+I0,0.2+I0),
           type='h',xlab='',ylab='',main=colnames(Returns[1+d]))
      abline(h=0,lwd=0.4)
    }
    
    
    par(mfrow=c(2,1),
        pty='m',
        mgp=c(2,0.5,0),
        omi=c(0,0,0,0),
        mar=c(3,4,0.1,0.1),las=1)
    plot(Returns[,1],Returns[,2],type='h',xlab='',ylab='Negative log-returns')
    plot(Returns[,1],log(Z0[,2]),type='h',xlab='',ylab='GARCH-Rescaled negative log-returns (log)',col=alpha(I0+8,0.2+I0))
  }
  
#  W0=W0[which(I0),]
  list(W=W0[which(I0),],R=R0,Z=Z0,X=Returns,I=I0,W0=W0)
}

generate=function(n,par,K,D){
  eta=as.numeric(par$eta)
  alpha=par$alpha
  m=colSums(eta*alpha/(rowSums(alpha)))
  X=P=W=U=Y=c()
  N=0
  while(N<n){
    y=t(rmultinom(1,size=1,prob=eta))
    a=alpha[which(y!=0),]
    x=rdirichlet(1,a)
    w=(x/m)/(sum(x/m)) 
    p=min(m/sum(m*w))
    u=runif(1)
    if(u<=p){N=N+1}
    P=c(P,p)
    U=c(U,u)
    X=rbind(X,x)
    W=rbind(W,w)
    Y=rbind(Y,y)
  }
  C=as.numeric((1:K)%*%t(Y))
  ind=order(W[,1])
  list(X=X[ind,],W=W[ind,],Y=Y[ind,],U=U[ind],P=P[ind],C=C[ind],I=(U[ind]<P[ind]))
}



colors=c('#a6cee3','#1f78b4','#b2df8a','#33a02c',
         '#fb9a99','#e31a1c','#fdbf6f','#ff7f00',
         '#cab2d6','#6a3d9a')
```

We run a short version of the MCMC algorithm (with $10000$ iteration) on the financial dataset introduced in the paper to compare the proposed tilted Dirichlet mixture approach to the Sabourin and Naveau (2014) algorithm. 

# Marginal modelling

We consider $D = 6$ time series of negative daily returns for indices in different sectors of the US market between June 2002 and June 2023, extracted from Yahoo Finance. The indices are computed based on various company stocks, which have been divided into six sectors depending on their main activity.

We fit an AR(1)-GARCH(1,1) distribution to each sector, then transform the standard- ized residuals to be unit FrÃ©chet. Based on the qq-plots, the models seem to be well-fitted to the bulk of the data but not to the tail, where extreme quantiles seem to be slightly larger than their theoretical counterparts. In both cases, we define $R=\sum_{d=1}^D Z_d$ and $W = Z /R$ , then select observations for which the radius exceeds its $0.97$th quantile. Hence, an extreme event could be due to an extreme loss in a single sector or a combination of moderate losses simultaneously in several sectors.

```{r,echo=FALSE}
sim=data_processing(plt=TRUE)
```

# Angular modelling

```{r,include=FALSE}
e0 = 2
a0=1000
b0=100
l0 = 1
s0 = 1
Kmin = 2
Kmax = 10

v0 = 1
u0=20
```

```{r,}
type='tdm'
run=TRUE
S =10^4
burnin = 0
thining.step=1

simu=u0+3
set.seed(4350)
```

We run a short MCMC for `r S` iterations.

```{r}
N = nrow(sim$W)
D = ncol(sim$W)


if(run==TRUE){
  #print(simu)
  time=rjmcmc_tdm(sim,simu,S,e0,u0,v0,a0,b0,l0,s0,Kmin,Kmax,thining.step,plt=FALSE,type=type,save.path = save.path)
  print(time)}
```

```{r,include=FALSE}
if (type == 'tdm') {
    Results <- read.csv(file.path(save.path,paste0('Results', D, '_', simu, '.csv')))
    Allocations <- read.csv(file.path(save.path,paste0('Allocations', D, '_', simu, '.csv')))
  }
  if (type == 'dm') {
    Results <- read.csv(file.path(save.path,paste0('Results', D, '_', simu, '_dm.csv')))
    Allocations <- read.csv(file.path(save.path,paste0('Allocations', D, '_', simu, '_dm.csv')))
  }
  
  uniqueResults = unique(Results[, c('iter', 'K', 'posterior', 'move')])
  uniqueResults=uniqueResults[uniqueResults$iter>0,]

  uniqueResults$llk=0
  likelihood = c()
  E = matrix(0, N, N)
  cE = list()
  
  for (k in 1:nrow(uniqueResults)) {
    s = uniqueResults$iter[k]
    Ks = unique(Results[Results$iter == s, 'K'])
    pars = list(  eta = as.numeric(Results[Results$iter == s, 6]),
                alpha = as.matrix(Results[Results$iter == s, 6 + (1:D)]))
    if (type == 'dm') {
      L = Llk_dm(sim$W, pars, N, Ks, D)
    }
    if (type == 'tdm') {
      L = Llk(sim$W, pars, N, Ks, D)
    }

    uniqueResults[k,'llk']=sum(log(rowSums(exp(L))))
    proba = exp(L) / rowSums(exp(L))
    A = matrix(1, N, N)
    for (i in 1:(N - 1))
    {
      for (j in (i + 1):N)
      {
        A[i, j] = A[j, i] = (proba[i, ] %*% proba[j, ])
      }
    }
    E = E + A
    cE[[k]] = A
    
  }

  E = E / nrow(uniqueResults)
  pE = E
  diag(pE) = 1
```

```{r,include=FALSE}



llk=lapply(1:nrow(uniqueResults),function(i)
  #for(i in 1:nrow(uniqueResults))
  {
    if(i<nrow(uniqueResults)){
      rep(uniqueResults$llk[i],uniqueResults$iter[i+1]-uniqueResults$iter[i])}
    else{
      rep(uniqueResults$llk[i],S-uniqueResults$iter[i])  
      
    }
  }
  )
  llk_serie=unlist(llk)
```

# Visual convergence diagnostics

```{r}
par(mfrow=c(1,1))
  plot(uniqueResults$iter,
       uniqueResults$llk,
       type = 'b',pch=20,
       xlab='iteration',
       ylab='log-likelihood',
       col=colors[uniqueResults$K],
       main = type)
```

**Acceptance ratio:**

```{r,include=TRUE}
print(100*nrow(uniqueResults)/S)
  print(table(alloc))
```

**Trace of the log-likelihood at each iteration of the MCMC chain (without thinning)**

```{r}
par(mfrow=c(1,1))
  plot(llk_serie,
       type = 'l',col=1,
       xlab='iteration',
       ylab='log-likelihood',
       #ylim=c(600,900),
       #,col=colors[uniqueResults$K],
       main = type)
```

**Distribution of the number of clusters throughout the MCMC chain (without thinning)**

```{r}
par(mfrow=c(1,1))
hist(uniqueResults$K,
       xlab = 'number of clusters',
       main=type
       )
```

**ACF and pACF of the log-likelihood at each iteration of the MCMC without thinning.**

```{r}
par(mfrow=c(1,2))
acf(llk_serie,main=paste0('ACF for ',type),100)
pacf(llk_serie,main=paste0('pACF for ',type),100)
```

**ACF and pACF of the log-likelihood after removing** $10\%$ as burn-in and thinning.

```{r}
thin=floor(seq(0.1*(length(llk_serie)),length(llk_serie),100))
llk_series_thin=llk_serie[thin]
par(mfrow=c(1,2))
acf(llk_series_thin,main=paste0('ACF for ',type, 'after thinning'),100)
pacf(llk_series_thin,main=paste0('pACF for ',type,'after thinning'),100)
```

# Visual representation of the clustered data (using the PEAR clustering)

```{r,include=FALSE}
uniqueResults$PEAR=1-pear(cls=Allocations[,-1],psm=pE)
uniqueResults_thin=uniqueResults[uniqueResults$iter%in%thin,]
choices=rbind(
    c( 'PEAR','2)Phat',as.numeric(uniqueResults[which.min(uniqueResults$PEAR),c('iter','K','PEAR')]))
  )
  choices=cbind(simu,D,choices)
  colnames(choices)=c('simu','dim','method','type','iter','K','criterion')
  choices=data.frame(choices)
  choices$iter=as.numeric(  choices$iter)
  choices$K=as.numeric(choices$K)
  choices$criterion=as.numeric(  choices$criterion)
```

```{r,include=FALSE}
methods=choices$method
  crit=1
  cl=choices[crit,'iter']
  alloc=as.numeric(Allocations[Allocations[,1]==cl,-1])
```

**Number of observations in each cluster using the PEAR clustering:**

```{r}
print(table(alloc))
```

**Pairs plots for the angular variables labelled by the clusters found using the PEAR loss function.**

```{r}
freq=data.frame(table(alloc))
    colnames(freq)=c('A','freq')
    freq=freq[order(freq$freq,decreasing = TRUE),]
    freq$rank=1:(nrow(freq))
    Al=data.frame(sim$W,A=alloc)
    Al=merge(Al,freq)
    pairs(Al[,2:(1+D)],col=colors[Al$rank],pch=20,lower.panel = NULL)
```

```{r}
dataX=data.frame(X=sim$W0,E=sim$I,date=sim$X[,1])
dataX$A=0
dataX$A[dataX$E]=alloc
freq=data.frame(table(dataX$A))
colnames(freq)=c('A','freq')
freq=freq[order(freq$freq,decreasing = TRUE),]
freq$rank=0:(nrow(freq)-1)
supp.labs <- paste0('Cluster ',freq$rank,' (size=',freq$freq,')')
names(supp.labs) <- freq$rank

colnames(dataX)=c(substring(colnames(sim$X)[-1],5),'E','date','A')
dataX=merge(dataX,freq)
dataX=dataX%>%
    pivot_longer(-c(freq,A,rank,date,E))%>%
    data.frame()

 p= ggplot(dataX)+
    geom_col(data=dataX[!dataX$E,],
             mapping=aes(x=as.Date(date),y=(value)),
             linewidth=0.1,alpha=0.3,col='grey')+
    geom_col(data=dataX[dataX$E,],mapping=aes(x=as.Date(date),y=(value),
                                              col=as.factor(rank)),
             linewidth=0.5,alpha=1)+
    scale_x_date(date_breaks = '2 years',expand=c(0.01,0,0.01,0),date_labels = '%Y')+
    facet_wrap(vars(name),ncol=1)+
    theme_bw()+theme(legend.position = 'none',text=element_text(size=15))+
    scale_color_manual(values=c(colors))+
    geom_hline(yintercept = 1/D,linetype=2)+
    labs(x='Time',y='')
 print(p)


```

```{r}

  p=ggplot(dataX[dataX$E,],
         aes(y=as.factor(name),x=value,fill=as.factor(rank)))+
    geom_vline(xintercept=(0:D)/D,linetype=1,color=8,linewidth=0.1)+
    geom_vline(xintercept=1/D,linetype=2)+
    geom_boxplot()+
    facet_wrap(vars(rank),labeller =labeller(rank=supp.labs),ncol=3)+
    theme_bw()+theme(legend.position = 'none',text=element_text(size=15),panel.grid = element_blank())+
    scale_fill_manual(values=colors)+
    labs(x='Angular variables',y='')
  print(p)

```

```{r}
cl=choices[1,'iter']
alloc=Allocations[which(uniqueResults$iter==cl),-1]
A=data.frame(t(alloc))
colnames(A)=c('PEAR')

pEl=data.frame(pE[order(A$PEAR),order(A$PEAR)])
colnames(pEl)=1:ncol(pEl)
pEl$x=1:nrow(pEl)

pEl=cbind(pEl,PEAR=A[order(A$PEAR),])
pEl=pEl%>%
  pivot_longer(-c(x,PEAR),
               names_to = 'y',
               names_prefix = 'X.')%>%
  data.frame()
pEl$y=as.numeric(pEl$y)
str(pEl)



freq=data.frame(table(A$PEAR))
colnames(freq)=c('PEAR','freq')
freq=freq[order(freq$freq,decreasing = TRUE),]
freq$rankP=1:nrow(freq)
Hf=merge(pEl,freq[,c('PEAR','rankP')])


p=ggplot(Hf[Hf$x<=Hf$y,],aes(x=x,y=y,fill=value))+
  geom_tile()+
  geom_point(aes(x=x+20,y=x-20,col=as.factor(rankP)),shape=15,size=2)+
  theme_bw()+
  guides(color='none')+
  theme(panel.grid = element_blank(),
        legend.position = 'top',
        aspect.ratio = 1,
        legend.key.width = unit(2,'cm'),
        axis.text = element_blank())+
  scale_color_manual(values=colors)+
  scale_fill_binned_sequential('Grays',breaks=seq(0,1,0.1),name='')+
  labs(x='',y='')

print(p)
```

We simulate 5000 points from a tilted Dirichlet mixture with the following parameters

```{r}
Ks = unique(Results[Results$iter == cl, 'K'])
    pars = list(  eta = as.numeric(Results[Results$iter == cl, 6]),
                alpha = as.matrix(Results[Results$iter == cl, 6 + (1:D)]))
  
pars
```

**pairs plot of the simulated data**

```{r}
n=5000
data=generate(n,pars,Ks,D)
```

```{r}
pairs(data$W[data$I,],pch=16)
```

**Marginal means of the simulated data:**

```{r}
colMeans(data$W[data$I,])
```



